{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
      "metadata": {
        "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc"
      },
      "source": [
        "# Deep Reinforcement Learning Laboratory\n",
        "\n",
        "In this laboratory session we will work on getting more advanced versions of Deep Reinforcement Learning algorithms up and running. Deep Reinforcement Learning is **hard**, and getting agents to stably train can be frustrating and requires quite a bit of subtlety in analysis of intermediate results. We will start by refactoring (a bit) my implementation of `REINFORCE` on the [Cartpole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f",
      "metadata": {
        "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f"
      },
      "source": [
        "## Exercise 1: Improving my `REINFORCE` Implementation (warm up)\n",
        "\n",
        "In this exercise we will refactor a bit and improve some aspects of my `REINFORCE` implementation.\n",
        "\n",
        "**First Things First**: Spend some time playing with the environment to make sure you understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6",
      "metadata": {
        "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import Categorical\n",
        "import wandb\n",
        "from collections import deque\n",
        "\n",
        "# Instantiate a rendering and a non-rendering environment.\n",
        "env_render = gym.make('CartPole-v1', render_mode='human')\n",
        "env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
        "outputId": "91eb4ecc-cc4e-4cbc-f86e-dc27013e2201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation after reset: [-0.00895654 -0.00242876 -0.04069757  0.03738886]\n",
            "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
          ]
        }
      ],
      "source": [
        "observation, info = env.reset()\n",
        "print(\"Observation after reset:\", observation)\n",
        "print(\"Observation space:\", env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae31dce-be71-4a86-95fd-1af902f026b8",
      "metadata": {
        "id": "0ae31dce-be71-4a86-95fd-1af902f026b8"
      },
      "source": [
        "**Next Things Next**: Now get your `REINFORCE` implementation working on the environment. You can import my (probably buggy and definitely inefficient) implementation here. Or even better, refactor an implementation into a separate package from which you can `import` the stuff you need here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "70490822-2859-4787-bf00-3084ad48252a",
      "metadata": {
        "id": "70490822-2859-4787-bf00-3084ad48252a"
      },
      "outputs": [],
      "source": [
        "# Given an environment, observation, and policy, sample from pi(a | obs). Returns the\n",
        "# selected action and the log probability of that action (needed for policy gradient).\n",
        "def select_action(env, obs, policy):\n",
        "    dist = Categorical(policy(obs))\n",
        "    action = dist.sample()\n",
        "    log_prob = dist.log_prob(action)\n",
        "    return (action.item(), log_prob.reshape(1))\n",
        "\n",
        "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
        "# .copy() the final numpy array. There's probably a better way to do this.\n",
        "def compute_returns(rewards, gamma):\n",
        "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
        "\n",
        "# Given an environment and a policy, run it up to the maximum number of steps.\n",
        "def run_episode(env, policy, maxlen=500):\n",
        "    # Collect just about everything.\n",
        "    observations = []\n",
        "    actions = []\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "\n",
        "    # Reset the environment and start the episode.\n",
        "    (obs, info) = env.reset()\n",
        "    for i in range(maxlen):\n",
        "        # Get the current observation, run the policy and select an action.\n",
        "        obs = torch.tensor(obs)\n",
        "        (action, log_prob) = select_action(env, obs, policy)\n",
        "        observations.append(obs)\n",
        "        actions.append(action)\n",
        "        log_probs.append(log_prob)\n",
        "\n",
        "        # Advance the episode by executing the selected action.\n",
        "        (obs, reward, term, trunc, info) = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        if term or trunc:\n",
        "            break\n",
        "    return (observations, actions, torch.cat(log_probs), rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0oIjWDbi4kAp",
      "metadata": {
        "id": "0oIjWDbi4kAp"
      },
      "outputs": [],
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
        "        self.fc2 = nn.Linear(128, env.action_space.n)\n",
        "\n",
        "    def forward(self, s):\n",
        "        s = F.relu(self.fc1(s))\n",
        "        s = F.softmax(self.fc2(s), dim=-1)\n",
        "        return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "87qA1WGY4nzl",
      "metadata": {
        "id": "87qA1WGY4nzl"
      },
      "outputs": [],
      "source": [
        "# The REINFORCE algorithm implementation.\n",
        "def reinforce(policy, env, gamma=0.99, num_episodes=10, eval_interval=100, eval_episodes=10, use_standardize_baseline=True, policy_lr=1e-2):\n",
        "    \n",
        "    opt = torch.optim.Adam(policy.parameters(), lr=policy_lr)\n",
        "\n",
        "    # Track episode rewards in a list.\n",
        "    scores_on_100_episodes = deque(maxlen=100) # Keep track of the last 100 episode scores\n",
        "    eval_avg_rewards = []\n",
        "    eval_avg_lengths = []  \n",
        "    \n",
        "\n",
        "    # The main training loop.\n",
        "    policy.train()\n",
        "    for episode in range(num_episodes):\n",
        "        # Run an episode of the environment, collect everything needed for policy update.\n",
        "        (observations, actions, log_probs, rewards) = run_episode(env, policy)\n",
        "\n",
        "        # Compute the discounted reward for every step of the episode.\n",
        "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
        "\n",
        "        scores_on_100_episodes.append(sum(rewards))\n",
        "        average_score = np.mean(scores_on_100_episodes)\n",
        "\n",
        "        # Standardize returns if the option is enabled.\n",
        "        if use_standardize_baseline:\n",
        "            returns = (returns - returns.mean()) / returns.std()\n",
        "\n",
        "        wandb.log({'episode_reward': sum(rewards), 'average_score_100':average_score}) # Log episode reward and average score over last 100 episodes\n",
        "        \n",
        "        # Make an optimization step\n",
        "        opt.zero_grad()\n",
        "        loss = (-log_probs * returns).mean()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        print(f'\\rEpisode {episode}\\tAverage Score: {average_score:.2f}', end=\"\")\n",
        "\n",
        "\n",
        "        # Perform evaluation periodically\n",
        "        if (episode + 1) % eval_interval == 0:\n",
        "            policy.eval()\n",
        "            avg_reward = 0\n",
        "            avg_length = 0\n",
        "            for _ in range(eval_episodes):\n",
        "                (eval_observations, eval_actions, eval_log_probs, eval_rewards) = run_episode(env, policy)\n",
        "                avg_reward += sum(eval_rewards)\n",
        "                avg_length += len(eval_rewards)\n",
        "            avg_reward /= eval_episodes\n",
        "            avg_length /= eval_episodes\n",
        "            eval_avg_rewards.append(avg_reward)\n",
        "            eval_avg_lengths.append(avg_length)\n",
        "            wandb.log({'eval_avg_reward': avg_reward, 'eval_avg_length': avg_length}) # Log evaluation metrics and run number\n",
        "            print(f'\\nEpisode {episode + 1}: Evaluation average reward: {avg_reward}')\n",
        "            print(f'Episode {episode + 1}: Evaluation average length: {avg_length}')\n",
        "            if avg_reward >= 500:\n",
        "                 torch.save(policy.state_dict(), \"CartPole_REINFORCE_no_baseline.pt\")\n",
        "                 break\n",
        "            policy.train()\n",
        "\n",
        "    # Return the running rewards, average evaluation rewards, and average evaluation lengths.\n",
        "    policy.eval()\n",
        "    return eval_avg_rewards, eval_avg_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0ea20317",
      "metadata": {},
      "outputs": [],
      "source": [
        "def reinforce_train():\n",
        "    wandb.init(group=\"cartpole_REINFORCE_No_Baseline\", name=\"REINFORCE_No_Baseline\")\n",
        "    config = wandb.config\n",
        "    \n",
        "    seed = 2000\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "    policy = PolicyNet(env)\n",
        "    \n",
        "    reinforce(\n",
        "        policy=policy,\n",
        "        env=env,\n",
        "        gamma=config.gamma,\n",
        "        num_episodes=config.num_episodes,\n",
        "        eval_interval=config.eval_interval,\n",
        "        eval_episodes=config.eval_episodes,\n",
        "        use_standardize_baseline=config.use_standardize_baseline,\n",
        "        policy_lr=config.policy_lr\n",
        "    )\n",
        "\n",
        "    env.close()\n",
        "    env_render.close()\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9d02a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {'name': 'eval_avg_reward', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'gamma': {'min': 0.98, 'max': 0.999},\n",
        "        'policy_lr': {'min': 1e-4, 'max': 1e-2},\n",
        "        'num_episodes': {'value': 2000},\n",
        "        'use_standardize_baseline': {'value': False},\n",
        "        'eval_interval': {'value': 100},\n",
        "        'eval_episodes': {'value': 10},\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Laboratory-2\")\n",
        "wandb.agent(sweep_id, function=reinforce_train, count=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ed7fad0b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/andromeda/personal/mpiras/miniconda3/envs/DRL/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /andromeda/personal/mpiras/Lab-2/cartpole_REINFORCE_no_baseline_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation episode reward: 500.0\n",
            "Evaluation episode reward: 500.0\n",
            "Evaluation episode reward: 500.0\n",
            "Evaluation episode reward: 500.0\n",
            "Evaluation episode reward: 500.0\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env = gym.wrappers.RecordVideo(env, video_folder=\"./cartpole_REINFORCE_no_baseline_videos\", episode_trigger=lambda x: True)\n",
        "policy = PolicyNet(env)\n",
        "policy.load_state_dict(torch.load(\"CartPole_REINFORCE_no_baseline.pt\"))\n",
        "policy.eval()\n",
        "for _ in range(5):\n",
        "    (eval_observations, eval_actions, eval_log_probs, eval_rewards) = run_episode(env, policy)\n",
        "    print(f'Evaluation episode reward: {sum(eval_rewards)}')\n",
        "env.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b8a90f-78ed-4f43-9d66-aeba7c74fe2c",
      "metadata": {
        "id": "18b8a90f-78ed-4f43-9d66-aeba7c74fe2c"
      },
      "source": [
        "**Last Things Last**: My implementation does a **super crappy** job of evaluating the agent performance during training. The running average is not a very good metric. Modify my implementation so that every $N$ iterations (make $N$ an argument to the training function) the agent is run for $M$ episodes in the environment. Collect and return: (1) The average **total** reward received over the $M$ iterations; and (2) the average episode length. Analyze the performance of your agents with these new metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c3xExU5VSyp",
      "metadata": {
        "id": "8c3xExU5VSyp"
      },
      "source": [
        "N.B for the \"vanilla\" cartpole enviroment the total reward and the lenght of the episode are the same"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
      "metadata": {
        "id": "2a5adad7-759b-4000-925b-701f41fe6e97"
      },
      "source": [
        "-----\n",
        "## Exercise 2: `REINFORCE` with a Value Baseline (warm up)\n",
        "\n",
        "In this exercise we will augment my implementation (or your own) of `REINFORCE` to subtract a baseline from the target in the update equation in order to stabilize (and hopefully speed-up) convergence. For now we will stick to the Cartpole environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07",
      "metadata": {
        "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07"
      },
      "source": [
        "**First Things First**: Recall from the slides on Deep Reinforcement Learning that we can **subtract** any function that doesn't depend on the current action from the q-value without changing the (maximum of our) objecttive function $J$:  \n",
        "\n",
        "$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) $$\n",
        "\n",
        "In `REINFORCE` this means we can subtract from our target $G_t$:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
        "\n",
        "Since we are only interested in the **maximum** of our objective, we can also **rescale** our target by any function that also doesn't depend on the action. A **simple baseline** which is even independent of the state -- that is, it is **constant** for each episode -- is to just **standardize rewards within the episode**. So, we **subtract** the average return and **divide** by the variance of returns:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha \\left(\\frac{G_t - \\bar{G}}{\\sigma_G}\\right) \\nabla  \\pi(A_t \\mid s, \\boldsymbol{\\theta}) $$\n",
        "\n",
        "This baseline is **already** implemented in my implementation of `REINFORCE`. Experiment with and without this standardization baseline and compare the performance. We are going to do something more interesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fbd26ce0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def reinforce_standardized_baseline_train():\n",
        "    wandb.init(group=\"Standardized_Baseline_cartpole_REINFORCE\", name=\"REINFORCE_Standardized_Baseline\")\n",
        "    config = wandb.config\n",
        "    \n",
        "    seed = 2000\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "    policy = PolicyNet(env)\n",
        "    \n",
        "    reinforce(\n",
        "        policy=policy,\n",
        "        env=env,\n",
        "        gamma=config.gamma,\n",
        "        num_episodes=config.num_episodes,\n",
        "        eval_interval=config.eval_interval,\n",
        "        eval_episodes=config.eval_episodes,\n",
        "        use_standardize_baseline=config.use_standardize_baseline,\n",
        "        policy_lr=config.policy_lr\n",
        "    )\n",
        "\n",
        "    env.close()\n",
        "    env_render.close()\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0262a617",
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {'name': 'eval_avg_reward', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'gamma': {'min': 0.98, 'max': 0.999},\n",
        "        'policy_lr': {'min': 1e-4, 'max': 1e-2},\n",
        "        'num_episodes': {'value': 2000},\n",
        "        'use_standardize_baseline': {'value': True}, # Use standardized returns as baseline\n",
        "        'eval_interval': {'value': 100},\n",
        "        'eval_episodes': {'value': 10},\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Laboratory-2\")\n",
        "wandb.agent(sweep_id, function=reinforce_standardized_baseline_train, count=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2d00039d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/andromeda/personal/mpiras/miniconda3/envs/DRL/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /andromeda/personal/mpiras/Lab-2/cartpole_REINFORCE_no_baseline_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n",
            "/andromeda/personal/mpiras/miniconda3/envs/DRL/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream, resource_exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation episode reward: 500.0\n",
            "Evaluation episode reward: 500.0\n",
            "Evaluation episode reward: 500.0\n",
            "Evaluation episode reward: 500.0\n",
            "Evaluation episode reward: 500.0\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env = gym.wrappers.RecordVideo(env, video_folder=\"./cartpole_REINFORCE_no_baseline_videos\", episode_trigger=lambda x: True)\n",
        "policy = PolicyNet(env)\n",
        "policy.load_state_dict(torch.load(\"CartPole_REINFORCE_no_baseline.pt\"))\n",
        "policy.eval()\n",
        "for _ in range(5):\n",
        "    (eval_observations, eval_actions, eval_log_probs, eval_rewards) = run_episode(env, policy)\n",
        "    print(f'Evaluation episode reward: {sum(eval_rewards)}')\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df",
      "metadata": {
        "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df"
      },
      "source": [
        "**The Real Exercise**: Standard practice is to use the state-value function $v(s)$ as a baseline. This is intuitively appealing -- we are more interested in updating out policy for returns that estimate the current **value** worse. Our new update becomes:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - \\tilde{v}(S_t \\mid \\mathbf{w})) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
        "\n",
        "where $\\tilde{v}(s \\mid \\mathbf{w})$ is a **deep neural network** with parameters $w$ that estimates $v_\\pi(s)$. What neural network? Typically, we use the **same** network architecture as that of the Policy.\n",
        "\n",
        "**Your Task**: Modify your implementation to fit a second, baseline network to estimate the value function and use it as **baseline**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "03bb9ebf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import Categorical\n",
        "import wandb\n",
        "\n",
        "\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
        "        self.fc2 = nn.Linear(128, 1) \n",
        "\n",
        "    def forward(self, s):\n",
        "        s = F.relu(self.fc1(s))\n",
        "        s = self.fc2(s)\n",
        "        return s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6fea17",
      "metadata": {},
      "outputs": [],
      "source": [
        "# The REINFORCE algorithm with baseline implementation, it is possible to choose whether to use the value baseline or standardized returns.\n",
        "def reinforce_with_baseline(policy, value_net, env, gamma=0.99, num_episodes=10, eval_interval=100, eval_episodes=10, use_value_baseline=True, policy_lr=1e-2, value_lr=1e-2):\n",
        "    \n",
        "    opt_policy = torch.optim.Adam(policy.parameters(), lr=policy_lr)\n",
        "    opt_value = torch.optim.Adam(value_net.parameters(), lr=value_lr)\n",
        "    value_criterion = nn.MSELoss()\n",
        "\n",
        "    # Track episode rewards in a list.\n",
        "    scores_on_100_episodes = deque(maxlen=100) # Keep track of the last 100 episode scores\n",
        "    eval_avg_rewards = []\n",
        "    eval_avg_lengths = []\n",
        "    best_eval_reward = -float('inf')  \n",
        "\n",
        "    # The main training loop.\n",
        "    policy.train()\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        (observations, actions, log_probs, rewards) = run_episode(env, policy)\n",
        "\n",
        "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
        "        observations_tensor = torch.stack(observations)\n",
        "\n",
        "        scores_on_100_episodes.append(sum(rewards))\n",
        "        average_score = np.mean(scores_on_100_episodes)\n",
        "\n",
        "        # Compute value loss and update value network\n",
        "        predicted_values = value_net(observations_tensor).squeeze()\n",
        "        value_loss = value_criterion(predicted_values, returns)\n",
        "        opt_value.zero_grad()\n",
        "        value_loss.backward()\n",
        "        opt_value.step()\n",
        "\n",
        "        # Compute advantages, if value baseline is not used, use standardized returns\n",
        "        if use_value_baseline:\n",
        "            advantages = returns - predicted_values.detach()\n",
        "        else:\n",
        "            advantages = (returns - returns.mean()) / returns.std()\n",
        "\n",
        "        wandb.log({'episode_reward': sum(rewards), 'average_score_100':average_score}) # Log episode reward and average score over last 100 episodes\n",
        "        # Make an optimization step for policy network\n",
        "        opt_policy.zero_grad()\n",
        "        policy_loss = (-log_probs * advantages).mean()\n",
        "        policy_loss.backward()\n",
        "        opt_policy.step()\n",
        "\n",
        "        # Perform evaluation periodically\n",
        "        if (episode + 1) % eval_interval == 0:\n",
        "            policy.eval()\n",
        "            avg_reward = 0\n",
        "            avg_length = 0\n",
        "            for _ in range(eval_episodes):\n",
        "                (eval_observations, eval_actions, eval_log_probs, eval_rewards) = run_episode(env, policy)\n",
        "                avg_reward += sum(eval_rewards)\n",
        "                avg_length += len(eval_rewards)\n",
        "            avg_reward /= eval_episodes\n",
        "            avg_length /= eval_episodes\n",
        "            eval_avg_rewards.append(avg_reward)\n",
        "            eval_avg_lengths.append(avg_length)\n",
        "            wandb.log({'eval_avg_reward': avg_reward, 'eval_avg_length': avg_length}) # Log evaluation metrics and run number\n",
        "            print(f'Episode {episode + 1}: Evaluation average reward: {avg_reward}')\n",
        "            print(f'Episode {episode + 1}: Evaluation average length: {avg_length}')\n",
        "            if avg_reward >= 500:\n",
        "                 torch.save(policy.state_dict(), \"best_policy_with_baseline.pt\")\n",
        "                 break\n",
        "            policy.train()\n",
        "\n",
        "    # Return the running rewards, average evaluation rewards, and average evaluation lengths.\n",
        "    policy.eval()\n",
        "    return eval_avg_rewards, eval_avg_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9711570e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def reinforce_value_baseline_train():\n",
        "    wandb.init(group=\"REINFORCE_Value_Baseline\", name=\"Value_Baseline_cartpole_REINFORCE\")\n",
        "    config = wandb.config\n",
        "    \n",
        "    seed = 2000\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    env = gym.wrappers.RecordVideo(gym.make('CartPole-v1', render_mode='rgb_array'), video_folder='./reinforce_cartpole_no_baseline_videos', episode_trigger=lambda x: x % 500 == 0)\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "    policy = PolicyNet(env)\n",
        "    value_net = ValueNet(env)\n",
        "    \n",
        "    reinforce_with_baseline(\n",
        "        policy=policy,\n",
        "        value_net=value_net,\n",
        "        env=env,\n",
        "        gamma=config.gamma,\n",
        "        num_episodes=config.num_episodes,\n",
        "        eval_interval=config.eval_interval,\n",
        "        eval_episodes=config.eval_episodes,\n",
        "        use_value_baseline=config.use_value_baseline,\n",
        "        policy_lr=config.policy_lr,\n",
        "        value_lr=config.value_lr\n",
        "    )\n",
        "\n",
        "    env.close()\n",
        "    env_render.close()\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64fbe8e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {'name': 'eval_avg_reward', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'gamma': {'min': 0.98, 'max': 0.999},\n",
        "        'policy_lr': {'min': 1e-4, 'max': 1e-2},\n",
        "        'value_lr': {'min': 1e-4, 'max': 1e-2},\n",
        "        'num_episodes': {'value': 2000},\n",
        "        'use_value_baseline': {'value': True}, # Use value baseline\n",
        "        'eval_interval': {'value': 100},\n",
        "        'eval_episodes': {'value': 10},\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Laboratory-2\")\n",
        "wandb.agent(sweep_id, function=reinforce_value_baseline_train, count=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
      "metadata": {
        "id": "64bf1447-d222-4b24-a357-5b7f9824390c"
      },
      "source": [
        "-----\n",
        "## Exercise 3: Going Deeper\n",
        "\n",
        "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
        "\n",
        "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
        "\n",
        "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
        "\n",
        "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
        "\n",
        "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
        "\n",
        "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest)\n",
        "\n",
        "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
        "\n",
        "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
        "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
        "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1002bb4f",
      "metadata": {},
      "source": [
        "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a85db16e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple\n",
        "import gymnasium as gym\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7e2613de",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self, state_size, action_size, seed = 42):\n",
        "    super(Network, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.fc1 = nn.Linear(state_size, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    return self.fc3(x)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c451cb6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "\n",
        "  def push(self, event):\n",
        "    self.memory.append(event)\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0]\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    experiences = random.sample(self.memory, k = batch_size)\n",
        "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
        "    return states, next_states, actions, rewards, dones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0b2af9d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "\n",
        "  def __init__(self, state_size, action_size, learning_rate , replay_buffer_size, minibatch_size, discount_factor=0.99, interpolation_parameter=1e-3):\n",
        "    self.device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
        "    self.memory = ReplayMemory(replay_buffer_size)\n",
        "    self.t_step = 0\n",
        "    self.minibatch_size = minibatch_size\n",
        "    self.discount_factor = discount_factor\n",
        "    self.interpolation_parameter = interpolation_parameter\n",
        "    #self.warmup_size = 400  # Ensure enough samples before learning\n",
        "\n",
        "    # For wandb logging\n",
        "    self.last_loss = 0\n",
        "    self.last_q_values = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.memory.push((state, action, reward, next_state, done))\n",
        "    self.t_step = (self.t_step + 1) % 4\n",
        "    if self.t_step == 0:\n",
        "      if len(self.memory.memory) > self.minibatch_size:\n",
        "      #if len(self.memory.memory) > self.warmup_size:\n",
        "        #experiences = self.memory.sample(100)\n",
        "        experiences = self.memory.sample(self.minibatch_size)\n",
        "        self.learn(experiences, self.discount_factor)\n",
        "\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    self.local_qnetwork.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state)\n",
        "    self.local_qnetwork.train()\n",
        "\n",
        "    # For logging\n",
        "    self.last_q_values = action_values.cpu().numpy()\n",
        "    \n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, next_states, actions, rewards, dones = experiences\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    self.last_loss = loss.item()  # For logging\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.soft_update(self.local_qnetwork, self.target_qnetwork, self.interpolation_parameter)\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "13d7fce3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_policy(agent, env, n_episodes=10, max_steps=1000, solve_score=200):\n",
        "    \"\"\"\n",
        "    Evaluate the agent's policy over n_episodes.\n",
        "    Returns True if all episodes achieve at least `solve_score`.\n",
        "    \"\"\"\n",
        "    all_solved = True\n",
        "    rewards_list = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        for t in range(max_steps):\n",
        "            # Use greedy policy\n",
        "            action = agent.act(state, epsilon=0.0)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        rewards_list.append(episode_reward)\n",
        "        if episode_reward < solve_score:\n",
        "            all_solved = False  # At least one episode not solved\n",
        "\n",
        "    print(f\"Evaluation rewards: {rewards_list}\")\n",
        "    return all_solved, rewards_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "24393e10",
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainDQNLunarLander():\n",
        "    wandb.init(group=\"DQN_LunarLander\", name=\"LunarLander_DQN\")\n",
        "    config = wandb.config\n",
        "\n",
        "    env = gym.make(config.env)\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    number_actions = env.action_space.n\n",
        "\n",
        "    agent = Agent(\n",
        "        state_size,\n",
        "        number_actions,\n",
        "        learning_rate=config.learning_rate,\n",
        "        replay_buffer_size=config.replay_buffer_size,\n",
        "        minibatch_size=config.minibatch_size,\n",
        "        discount_factor=config.discount_factor,\n",
        "        interpolation_parameter=config.interpolation_parameter\n",
        "    )\n",
        "\n",
        "    episodes = config.episodes\n",
        "    max_steps = config.max_timesteps\n",
        "\n",
        "    epsilon = config.epsilon_start\n",
        "    epsilon_min = config.epsilon_end\n",
        "    epsilon_decay = config.epsilon_decay\n",
        "\n",
        "    scores_100 = deque(maxlen=100)\n",
        "\n",
        "    for episode in range(1, episodes + 1):\n",
        "\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            action = agent.act(state, epsilon)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
        "\n",
        "        scores_100.append(score)\n",
        "        avg_score = np.mean(scores_100)\n",
        "\n",
        "        wandb.log({\n",
        "            \"episode_reward\": score,\n",
        "            \"epsilon\": epsilon,\n",
        "            \"avg_score_100\": avg_score\n",
        "        })\n",
        "\n",
        "        print(f\"\\rEpisode {episode}  Avg Score: {avg_score:.2f}\", end=\"\")\n",
        "        if episode % 100 == 0:\n",
        "            print()\n",
        "\n",
        "        if avg_score >= 200:\n",
        "            #perform evaluation\n",
        "            all_solved, rewards_list = evaluate_policy(agent, env, n_episodes=10, max_steps=max_steps)\n",
        "            if all_solved:\n",
        "                print(f\"\\nSolved environment in {episode - 100} episodes (all evaluation episodes â‰¥ 200)!\")\n",
        "                torch.save(agent.local_qnetwork.state_dict(),\n",
        "                           f'checkpointLunarLander_solved_in_{episode-100}_episodes.pth')\n",
        "                break\n",
        "            else:\n",
        "                print(\"\\nEvaluation failed: not all episodes reached 200 reward. Continuing training...\")\n",
        "\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dfb118e",
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"grid\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"avg_score_100\",\n",
        "        \"goal\": \"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [5e-4, 1e-3]\n",
        "        },\n",
        "        \"discount_factor\": {\n",
        "            \"values\": [0.98, 0.99]\n",
        "        },\n",
        "        \"epsilon_decay\": {\n",
        "            \"values\": [0.995, 0.998]\n",
        "        },\n",
        "        \"interpolation_parameter\": {\n",
        "            \"values\": [1e-3, 1e-2]\n",
        "        },\n",
        "        # fixed parameters\n",
        "        \"replay_buffer_size\": {\"value\": 100000},\n",
        "        \"minibatch_size\": {\"value\": 128},\n",
        "        \"env\": {\"value\": \"LunarLander-v3\"},\n",
        "        \"episodes\": {\"value\": 1500},\n",
        "        \"max_timesteps\": {\"value\": 1000},\n",
        "        \"epsilon_start\": {\"value\": 1.0},\n",
        "        \"epsilon_end\": {\"value\": 0.01}\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Laboratory-2\")\n",
        "wandb.agent(sweep_id, function=trainDQNLunarLander)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "d67f8572",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dqn-eval-video</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/o3k2f94k' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/o3k2f94k</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251204_215311-o3k2f94k/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab-2/wandb/run-20251204_215341-rj36qoc1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/rj36qoc1' target=\"_blank\">dqn-eval-video</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/rj36qoc1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/rj36qoc1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/andromeda/personal/mpiras/miniconda3/envs/DRL/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /andromeda/personal/mpiras/Lab-2/LunarLander_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: reward = 264.93888747506355\n",
            "Episode 2: reward = 241.35734634056504\n",
            "Episode 3: reward = 266.1384300416239\n",
            "Episode 4: reward = 254.9475277884772\n",
            "Episode 5: reward = 230.78907007580221\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode_1_reward</td><td>â–</td></tr><tr><td>episode_2_reward</td><td>â–</td></tr><tr><td>episode_3_reward</td><td>â–</td></tr><tr><td>episode_4_reward</td><td>â–</td></tr><tr><td>episode_5_reward</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode_1_reward</td><td>264.93889</td></tr><tr><td>episode_2_reward</td><td>241.35735</td></tr><tr><td>episode_3_reward</td><td>266.13843</td></tr><tr><td>episode_4_reward</td><td>254.94753</td></tr><tr><td>episode_5_reward</td><td>230.78907</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dqn-eval-video</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/rj36qoc1' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/rj36qoc1</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2</a><br>Synced 4 W&B file(s), 5 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251204_215341-rj36qoc1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import wandb\n",
        "import glob\n",
        "\n",
        "# --- Init W&B ---\n",
        "wandb.init(\n",
        "    project=\"Laboratory-2\",\n",
        "    name=\"dqn-eval-video\",\n",
        "    config={\"num_eval_episodes\": 5}\n",
        ")\n",
        "\n",
        "env = gym.make(\n",
        "    \"LunarLander-v3\",\n",
        "    render_mode=\"rgb_array\"\n",
        ")\n",
        "\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env,\n",
        "    video_folder=\"LunarLander_videos\",\n",
        "    name_prefix=\"dqn-eval\",\n",
        "    episode_trigger=lambda episode_id: True\n",
        ")\n",
        "\n",
        "agent = Agent(\n",
        "    state_size=env.observation_space.shape[0],\n",
        "    action_size=env.action_space.n,\n",
        "    learning_rate=5e-3,\n",
        "    replay_buffer_size=int(1e5),\n",
        "    minibatch_size=64\n",
        ")\n",
        "\n",
        "agent.local_qnetwork.load_state_dict(\n",
        "    torch.load(\"DQL_LunarLander_checkpoints/checkpointLunarLander_solved_in_503_episodes.pth\")\n",
        ")\n",
        "agent.local_qnetwork.eval()\n",
        "\n",
        "num_eval_episodes = 5\n",
        "\n",
        "for ep in range(num_eval_episodes):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon=0.0)\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done or truncated:\n",
        "            print(f\"Episode {ep+1}: reward = {total_reward}\")\n",
        "            wandb.log({f\"episode_{ep+1}_reward\": total_reward})\n",
        "            break\n",
        "\n",
        "env.close()\n",
        "\n",
        "# --- Log videos to W&B ---\n",
        "video_files = sorted(glob.glob(\"LunarLander_videos/*.mp4\"))\n",
        "for vf in video_files:\n",
        "    wandb.log({\"eval/video\": wandb.Video(vf, fps=30, format=\"gif\")})\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "744de67e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainDQNCartPole():\n",
        "    wandb.init(group=\"DQN_CartPole\", name=\"CartPole_DQN\")\n",
        "    config = wandb.config\n",
        "\n",
        "    env = gym.make(config.env)\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    number_actions = env.action_space.n\n",
        "\n",
        "    agent = Agent(\n",
        "        state_size,\n",
        "        number_actions,\n",
        "        learning_rate=config.learning_rate,\n",
        "        replay_buffer_size=config.replay_buffer_size,\n",
        "        minibatch_size=config.minibatch_size,\n",
        "        discount_factor=config.discount_factor,\n",
        "        interpolation_parameter=config.interpolation_parameter\n",
        "    )\n",
        "\n",
        "    episodes = config.episodes\n",
        "    max_steps = config.max_timesteps\n",
        "\n",
        "    epsilon = config.epsilon_start\n",
        "    epsilon_min = config.epsilon_end\n",
        "    tau = config.tau\n",
        "\n",
        "    scores_100 = deque(maxlen=100)\n",
        "\n",
        "    for episode in range(1, episodes + 1):\n",
        "\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            action = agent.act(state, epsilon)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        #epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
        "        epsilon = epsilon_min + (config.epsilon_start - epsilon_min) * np.exp(-1. * episode / tau)\n",
        "\n",
        "        scores_100.append(score)\n",
        "        avg_score = np.mean(scores_100)\n",
        "\n",
        "        wandb.log({\n",
        "            \"episode_reward\": score,\n",
        "            \"epsilon\": epsilon,\n",
        "            \"avg_score_100\": avg_score\n",
        "        })\n",
        "\n",
        "        print(f\"\\rEpisode {episode}  Avg Score: {avg_score:.2f}\", end=\"\")\n",
        "        #if episode % 100 == 0:\n",
        "        #    print()\n",
        "\n",
        "        if avg_score >= 500:\n",
        "            print(f'\\nEnvironment solved in {episode - 100:d} episodes!\\tAverage Score: {avg_score:.2f}')\n",
        "            torch.save(agent.local_qnetwork.state_dict(), f'checkpointCartPole_solved_in_{episode-100}_episodes.pth')\n",
        "            break\n",
        "\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e87554ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"grid\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"avg_score_100\",\n",
        "        \"goal\": \"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [5e-4, 1e-3]\n",
        "        },\n",
        "        \"tau\": {\n",
        "            \"values\": [200, 300]\n",
        "        },\n",
        "        \"interpolation_parameter\": {\n",
        "            \"values\": [5e-3, 1e-2]\n",
        "        },\n",
        "        \"epsilon_end\": {\"values\": [0.01, 0.05]},\n",
        "\n",
        "        # fixed parameters\n",
        "        \"replay_buffer_size\": {\"value\": 100000},\n",
        "        \"minibatch_size\": {\"value\": 128},\n",
        "        \"env\": {\"value\": \"CartPole-v1\"},\n",
        "        \"episodes\": {\"value\": 1500},\n",
        "        \"max_timesteps\": {\"value\": 500},\n",
        "        \"epsilon_start\": {\"value\": 1.0},\n",
        "        \"discount_factor\": {\"value\": 0.99},\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Laboratory-2\")\n",
        "wandb.agent(sweep_id, function=trainDQNCartPole)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "cf622ff2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/andromeda/personal/mpiras/Lab-2/wandb/run-20251204_214739-4zd8lr75</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/4zd8lr75' target=\"_blank\">dqn-cartpole-eval-video</a></strong> to <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/4zd8lr75' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/4zd8lr75</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/andromeda/personal/mpiras/miniconda3/envs/DRL/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /andromeda/personal/mpiras/Lab-2/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: reward = 500.0\n",
            "Episode 2: reward = 500.0\n",
            "Episode 3: reward = 500.0\n",
            "Episode 4: reward = 500.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode_1_reward</td><td>â–</td></tr><tr><td>episode_2_reward</td><td>â–</td></tr><tr><td>episode_3_reward</td><td>â–</td></tr><tr><td>episode_4_reward</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode_1_reward</td><td>500</td></tr><tr><td>episode_2_reward</td><td>500</td></tr><tr><td>episode_3_reward</td><td>500</td></tr><tr><td>episode_4_reward</td><td>500</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dqn-cartpole-eval-video</strong> at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/4zd8lr75' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2/runs/4zd8lr75</a><br> View project at: <a href='https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2' target=\"_blank\">https://wandb.ai/matteo-piras-universit-di-firenze/Laboratory-2</a><br>Synced 4 W&B file(s), 5 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251204_214739-4zd8lr75/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import wandb\n",
        "import glob\n",
        "\n",
        "# ---- Init W&B ----\n",
        "wandb.init(\n",
        "    project=\"laboratory-2\",\n",
        "    name=\"dqn-cartpole-eval-video\",\n",
        "    config={\"num_eval_episodes\": 4}\n",
        ")\n",
        "\n",
        "env = gym.make(\n",
        "    \"CartPole-v1\",\n",
        "    render_mode=\"rgb_array\"\n",
        ")\n",
        "\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env,\n",
        "    video_folder=\"videos\",\n",
        "    name_prefix=\"dqn-eval\",\n",
        "    episode_trigger=lambda episode_id: True  # record ALL episodes\n",
        ")\n",
        "\n",
        "agent = Agent(\n",
        "    state_size=4,\n",
        "    action_size=2,\n",
        "    learning_rate=5e-3,\n",
        "    replay_buffer_size=int(1e5),\n",
        "    minibatch_size=64\n",
        ")\n",
        "\n",
        "agent.local_qnetwork.load_state_dict(torch.load(\"DQL_CartPole_checkpoints/checkpointCartPole_solved_in_1180_episodes.pth\"))\n",
        "agent.local_qnetwork.eval()\n",
        "\n",
        "num_eval_episodes = 4\n",
        "\n",
        "for ep in range(num_eval_episodes):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # deterministic action (no epsilon)\n",
        "        action = agent.act(state, epsilon=0.0)\n",
        "\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done or truncated:\n",
        "            print(f\"Episode {ep+1}: reward = {total_reward}\")\n",
        "            wandb.log({f\"episode_{ep+1}_reward\": total_reward})\n",
        "            break\n",
        "\n",
        "env.close()\n",
        "\n",
        "# ---- Log videos to W&B ----\n",
        "video_files = sorted(glob.glob(\"videos/*.mp4\"))\n",
        "\n",
        "for vf in video_files:\n",
        "    wandb.log({\"eval/video\": wandb.Video(vf, fps=30, format=\"gif\")})\n",
        "\n",
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DRL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
