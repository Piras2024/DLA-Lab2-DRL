# Given an environment, observation, and policy, sample from pi(a | obs). Returns the
# selected action and the log probability of that action (needed for policy gradient).
def select_action(env, obs, policy):
    dist = Categorical(policy(obs))
    action = dist.sample()
    log_prob = dist.log_prob(action)
    return (action.item(), log_prob.reshape(1))

# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to
# .copy() the final numpy array. There's probably a better way to do this.
def compute_returns(rewards, gamma):
    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()

# Given an environment and a policy, run it up to the maximum number of steps.
def run_episode(env, policy, maxlen=500):
    # Collect just about everything.
    observations = []
    actions = []
    log_probs = []
    rewards = []

    # Reset the environment and start the episode.
    (obs, info) = env.reset()
    for i in range(maxlen):
        # Get the current observation, run the policy and select an action.
        obs = torch.tensor(obs)
        (action, log_prob) = select_action(env, obs, policy)
        observations.append(obs)
        actions.append(action)
        log_probs.append(log_prob)

        # Advance the episode by executing the selected action.
        (obs, reward, term, trunc, info) = env.step(action)
        rewards.append(reward)
        if term or trunc:
            break
    return (observations, actions, torch.cat(log_probs), rewards)





    # REINFORCE policy gradient algorithm with optional value baseline.
def reinforce_with_baseline(policy, value_net, env, gamma=0.99, num_episodes=10, eval_interval=100, eval_episodes=10, use_value_baseline=True, lr_policy=1e-2, lr_value=2e-2, lr_scheduler_step_size=100, lr_scheduler_gamma=0.9):
    opt_policy = torch.optim.Adam(policy.parameters(), lr=lr_policy)
    opt_value = torch.optim.Adam(value_net.parameters(), lr=lr_value)
    value_criterion = nn.MSELoss()

    # Linear decay scheduler for learning rates
    scheduler_policy = torch.optim.lr_scheduler.LinearLR(opt_policy, start_factor=1.0, end_factor=0.0, total_iters=num_episodes)
    scheduler_value = torch.optim.lr_scheduler.LinearLR(opt_value, start_factor=1.0, end_factor=0.0, total_iters=num_episodes)


    running_rewards = [0.0]
    eval_avg_rewards = []
    eval_avg_lengths = []

    policy.train()
    value_net.train()
    for episode in range(num_episodes):
        (observations, actions, log_probs, rewards) = run_episode(env, policy)

        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)
        observations_tensor = torch.stack(observations)

        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])

        # Compute value loss and update value network
        predicted_values = value_net(observations_tensor).squeeze()
        value_loss = value_criterion(predicted_values, returns)
        opt_value.zero_grad()
        value_loss.backward()
        opt_value.step()

        # Compute advantages, if value baseline is not used, use standardized returns
        if use_value_baseline:
            advantages = returns - predicted_values.detach()
        else:
            advantages = (returns - returns.mean()) / returns.std()


        wandb.log({'episode_reward': sum(rewards)})
        # Make an optimization step for policy network
        opt_policy.zero_grad()
        policy_loss = (-log_probs * advantages).mean()
        policy_loss.backward()
        opt_policy.step()

        # Step the schedulers
        scheduler_policy.step()
        scheduler_value.step()


        # Perform evaluation periodically
        if (episode + 1) % eval_interval == 0:
            policy.eval()
            avg_reward = 0
            avg_length = 0
            for _ in range(eval_episodes):
                (eval_observations, eval_actions, eval_log_probs, eval_rewards) = run_episode(env, policy)
                avg_reward += sum(eval_rewards)
                avg_length += len(eval_rewards)
            avg_reward /= eval_episodes
            avg_length /= eval_episodes
            if avg_reward > best_eval_reward:
                 best_eval_reward = avg_reward
                 best_eval_episode = episode + 1  # episodes are 0-indexed
                 torch.save(policy.state_dict(), "best_policy.pt")
            # artifact = wandb.Artifact('best_policy', type='model')
            # artifact.add_file('best_policy.pt')
            # wandb.log_artifact(artifact)
            eval_avg_rewards.append(avg_reward)
            eval_avg_lengths.append(avg_length)
            wandb.log({'eval_avg_reward': avg_reward, 'eval_avg_length': avg_length, 'best_eval_reward': best_eval_reward, 'best_eval_episode': best_eval_episode}) # Log evaluation metrics and run number
            print(f'Episode {episode + 1}: Evaluation average reward: {avg_reward}')
            print(f'Episode {episode + 1}: Evaluation average length: {avg_length}')
            policy.train()

    policy.eval()
    value_net.eval()
    return running_rewards, eval_avg_rewards, eval_avg_lengths